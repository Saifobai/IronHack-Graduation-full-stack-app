# -*- coding: utf-8 -*-
"""ClipoFrameAi_10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XQXd4-K9IZpo6QtEpNYr4fzmD_arnPz6
"""

!pip install "pinecone-client<3" --force-reinstall

import pinecone
print(pinecone.__version__)  # should show 2.x

!pip install langchain-pinecone

!pip install -U langchain-openai

!pip install sentence-transformers transformers==4.34.0

# Commented out IPython magic to ensure Python compatibility.
# %pip install yt-dlp

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U openai-whisper

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain-community langchain

!pip install -U "pinecone-client>=2.2.1"

!pip install pinecone

!apt-get update -y && apt-get install -y ffmpeg

!pip install tiktoken moviepy huggingface_hub gradio

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U peft

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U transformers

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U gradio

!pip install --upgrade --force-reinstall \
  "numpy<2.0" \
  "numba>=0.59" \
  "pinecone-client<3" \
  "langchain==0.0.350" \
  "langchain-openai" \
  "openai-whisper" \
  "tiktoken" \
  "sentence-transformers" \
  "huggingface_hub"

!pip install pinecone

!pip install whisper

import numpy, numba, pinecone, langchain, whisper
print("NumPy:", numpy.__version__)
print("Numba:", numba.__version__)
print("Pinecone:", pinecone.__version__)
print("LangChain:", langchain.__version__)
print("Whisper:", whisper.__version__)

!pip install --upgrade langchain-community langchain-openai langchain-pinecone

# Core
import os, time, json, uuid, subprocess
from datetime import datetime
from typing import List, Dict, Any

# LangChain core
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.agents import initialize_agent, Tool
from langchain.memory import ConversationBufferMemory

# New split packages
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore as PineconeStore
from langchain.schema import Document

# sentence-transformers (fallback)
from sentence_transformers import SentenceTransformer

# Hugging Face helpers
from huggingface_hub import HfApi, login, Repository

# Whisper
import whisper

# Pinecone v3
from pinecone import Pinecone as PineconeClient

print("✅ Imports ready.")

# Core
import os, time, json, uuid, subprocess
from datetime import datetime
from typing import List, Dict, Any

# LangChain core
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.agents import initialize_agent, Tool
from langchain.memory import ConversationBufferMemory

# New split packages
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore as PineconeStore
from langchain.schema import Document

# sentence-transformers (fallback)
from sentence_transformers import SentenceTransformer

# Hugging Face helpers
from huggingface_hub import HfApi, login, Repository

# Whisper
import whisper

# Pinecone v3
from pinecone import Pinecone as PineconeClient

print("✅ Imports ready.")

!pip uninstall -y pinecone-client pinecone
!pip install pinecone

!pip install -U "pinecone-client>=3.0.0" --force-reinstall

!pip install -U langchain langchain-core

!pip install "pinecone-client<3" --force-reinstall
!pip install "numpy<2.0" --force-reinstall
!pip install numba==0.60 --force-reinstall

!pip install langchain-pinecone==0.1.0 --force-reinstall

!pip install "pinecone-client<3" --force-reinstall
!pip install "numpy<2.0" --force-reinstall
!pip install numba==0.57

!pip install "numba==0.57" --force-reinstall

# ============================================
# STEP 2 — Load API keys (Colab userdata or env fallback)
# ============================================
import os
import openai  # Make sure openai is imported

try:
    from google.colab import userdata
    OPENAI_API_KEY = userdata.get("OPENAI_API_KEY") or os.environ.get("OPENAI_API_KEY")
    PINECONE_API_KEY = userdata.get("PINECONE_API_KEY") or os.environ.get("PINECONE_API_KEY")
    HF_TOKEN = userdata.get("HF_TOKEN") or os.environ.get("HF_TOKEN")
    LANGCHAIN_API_KEY = userdata.get("LANGCHAIN_API_KEY") or os.environ.get("LANGCHAIN_API_KEY")
    LANGSMITH_API_KEY = userdata.get("LANGSMITH_API_KEY") or os.environ.get("LANGSMITH_API_KEY")
except Exception:
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    PINECONE_API_KEY = os.environ.get("PINECONE_API_KEY")
    HF_TOKEN = os.environ.get("HF_TOKEN")
    LANGCHAIN_API_KEY = os.environ.get("LANGCHAIN_API_KEY")
    LANGSMITH_API_KEY = os.environ.get("LANGSMITH_API_KEY")

# ✅ Enable LangSmith tracing
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
if LANGSMITH_API_KEY:
    os.environ["LANGCHAIN_API_KEY"] = LANGSMITH_API_KEY  # LangSmith uses this
os.environ["LANGCHAIN_PROJECT"] = "jarvis_demo"  # optional project grouping

# ✅ OpenAI client setup
openai.api_key = OPENAI_API_KEY

print("Keys loaded:")
print(" - OpenAI:", "✔" if OPENAI_API_KEY else "❌")
print(" - Pinecone:", "✔" if PINECONE_API_KEY else "❌")
print(" - HF Token:", "✔" if HF_TOKEN else "❌")
print(" - LangChain API:", "✔" if LANGCHAIN_API_KEY else "❌")
print(" - LangSmith:", "✔" if LANGSMITH_API_KEY else "❌")

# ============================================
# STEP a-a Initialize LangSmith client
# ============================================
from langsmith import Client as LangSmithClient


ls_client = None
try:
    ls_client = LangSmithClient(api_key=LANGSMITH_API_KEY)
    print("✅ LangSmith Client initialized")
except Exception as e:
    print("⚠️ LangSmith initialization failed:", e)
    ls_client = None

# ============================================
# STEP 3 — Safe Pinecone initialization (v3 client)
# ============================================
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.schema import Document
from pinecone import Pinecone

# Init Pinecone v3
pc = Pinecone(api_key=PINECONE_API_KEY)

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=OPENAI_API_KEY
)
index_name = "clipoframeaiapp04"  # your existing index

# --- Storage + Retrieval (per-video namespace) ---
def store_context(video_id: str, text: str):
    docs = [Document(page_content=text, metadata={"video_id": video_id})]
    vectorstore = PineconeVectorStore.from_documents(
        docs,
        embedding=embeddings,
        index_name=index_name,
        namespace=video_id  # 👈 ensures docs go into the right namespace
    )
    print(f"[store_context] Stored {len(docs)} docs under namespace={video_id}")



def retrieve_context(video_id: str, query: str = "Extract key highlights", k: int = 20):
    try:
        index = pc.Index(index_name)

        # Embed the query
        query_vec = embeddings.embed_query(query)

        # Query Pinecone directly
        res = index.query(
            namespace=video_id,
            vector=query_vec,
            top_k=k,
            include_metadata=True
        )

        matches = res.get("matches", [])
        print(f"[retrieve_context] Retrieved {len(matches)} docs under namespace={video_id} with query='{query}'")

        return [m["metadata"]["text"] for m in matches if "metadata" in m and "text" in m["metadata"]]
    except Exception as e:
        print("retrieve_context error:", e)
        return []

!pip uninstall -y whisper
!pip install -U openai-whisper

# ============================================
# STEP 4 — Whisper model
# ============================================
import torch
import subprocess
import os # Import os as it's used in download_youtube_audio
import whisper # Import whisper as it's used in this cell


device = "cuda" if torch.cuda.is_available() else "cpu"
WHISPER_MODEL = os.environ.get("WHISPER_MODEL", "small")
whisper_model = whisper.load_model(WHISPER_MODEL)


# ============================================
# STEP 5 — YouTube audio downloader helper (with cookies + refresh hint)
# ============================================
import subprocess, os, shutil

def download_youtube_audio(url, out="yt_audio.mp3", cookies="cookies.txt"):
    cmd = ["yt-dlp", "-f", "bestaudio", "--extract-audio", "--audio-format", "mp3", "-o", out, url]

    if cookies and os.path.exists(cookies):
        cmd.insert(1, "--cookies"); cmd.insert(2, cookies)
        print("✅ Using cookies.txt for authentication")
    else:
        print("⚠️ No cookies.txt found, downloading without login")

    try:
        result = subprocess.run(cmd, check=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return out
    except subprocess.CalledProcessError as e:
        if "Login required" in e.stderr or "cookies" in e.stderr.lower():
            print("❌ Download failed: Your cookies.txt may have expired. Please refresh it.")
        else:
            print(f"❌ Download failed: {e.stderr}")
        raise

def refresh_cookies(new_cookie_file="cookies.txt"):
    """Replace old cookies.txt with a fresh one"""
    if os.path.exists(new_cookie_file):
        shutil.copy(new_cookie_file, "cookies.txt")
        print("✅ Cookies refreshed and ready")
    else:
        print("⚠️ Provided cookie file not found")

# ============================================
# STEP 6 — Transcription helpers
# ============================================
def transcribe_file(file_path, language=None):
    if language:
        result = whisper_model.transcribe(file_path, language=language)
    else:
        result = whisper_model.transcribe(file_path)
    text = result.get("text", "")
    segments = result.get("segments", [])
    return text, segments

# ============================================
# STEP 7 — OpenAI / GPT-4 wrappers
# ============================================
from openai import OpenAI
client = OpenAI(api_key=OPENAI_API_KEY)


def gpt_chat(prompt, system="You are a helpful assistant.", max_tokens=600, temperature=0.2, model_name="gpt-4o-mini"):

  resp = client.chat.completions.create(
  model=model_name,
  messages=[{"role": "system", "content": system},{"role": "user", "content": prompt}],
  max_tokens=max_tokens, temperature=temperature)
  return resp.choices[0].message.content


def gpt4_chat(prompt, system="You are a helpful assistant.", max_tokens=600, temperature=0.2):
 return gpt_chat(prompt, system=system, max_tokens=max_tokens, temperature=temperature, model_name="gpt-4o")

# ============================================
# STEP 8 — Summarize / Highlights / Viral ideas
# ============================================
import re
import html as html_lib
def summarize_transcript(transcript):
    prompt = f"Summarize the transcript below into concise bullets or 3 short paragraphs:\n\n{transcript}"
    return gpt4_chat(prompt, system="You are a friendly summarization assistant.", max_tokens=600)

def generate_viral_ideas(transcript):
    prompt = f"Propose 3 viral clip ideas for TikTok/YouTube Shorts from this transcript. For each: title, start_time, end_time, 100-char caption and hashtags. Return JSON array.\n\n{transcript}"
    return gpt4_chat(prompt, system="You are a creative viral content writer.", max_tokens=700)

# Robust highlights parsing + HTML
def safe_json_parse(output):
    try:
        return json.loads(output)
    except Exception:
        pass
    m = re.search(r"\[(?:.|\n)*\]", output)
    if m:
        try:
            return json.loads(m.group(0))
        except Exception:
            pass
    lines = [l.strip() for l in output.splitlines() if l.strip()]
    items = []
    for line in lines:
        m = re.search(r"(?:\[\d+\]\s*)?([0-9]+(?:\.[0-9]+)?)\s*[-–]\s*([0-9]+(?:\.[0-9]+)?)s?:?\s*(.+)$", line)
        if m:
            start = float(m.group(1)); end = float(m.group(2)); txt = m.group(3).strip()
            items.append({"start": start, "end": end, "text": txt})
    if items:
        return items
    return {"error": "failed_to_parse", "raw": output}

def highlights_to_html(highlights):
    if not highlights:
        return "<i>No highlights found.</i>"
    if isinstance(highlights, dict) and highlights.get("error"):
        return f"<pre>{html_lib.escape(highlights.get('raw',''))}</pre>"
    if isinstance(highlights, dict):
        highlights = [highlights]
    html_lines = ['<div style="font-family: Inter, Arial, sans-serif;">']
    for it in highlights:
        try:
            start = float(it.get("start", 0.0))
            end = float(it.get("end", 0.0))
            txt = html_lib.escape(it.get("text", "") or "")
            html_lines.append(f'<div style="margin-bottom:8px;">⏱ <strong>{start:.1f}s - {end:.1f}s</strong> — {txt}</div>')
        except Exception:
            html_lines.append(f"<div>{html_lib.escape(str(it))}</div>")
    html_lines.append("</div>")
    return "\n".join(html_lines)

def call_gpt_for_highlights(segments):
    block = "\n".join([f"[{i}] {seg['start']:.1f}-{seg['end']:.1f}s: {seg['text']}" for i, seg in enumerate(segments)])
    prompt = (
        "From the numbered segments below, extract the top 5 highlights. "
        "For each highlight return a JSON object with keys: start (seconds), end (seconds), text (one-sentence reason). "
        "Return ONLY a JSON array. Example:\n"
        '[{"start":12.3,"end":14.5,"text":"..."}]\n\n'
        f"{block}"
    )
    out = gpt4_chat(prompt, system="You extract short impactful highlights with timestamps.", max_tokens=600)
    parsed = safe_json_parse(out)
    return parsed

def extract_highlights_with_timestamps(segments):
    return call_gpt_for_highlights(segments)

# ============================================
# STEP 9 — Embeddings & Pinecone storage
# ============================================
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
from pinecone import Pinecone
import hashlib
from datetime import datetime  # ✅ needed for store_fact

# Init Pinecone v3
pc = Pinecone(api_key=PINECONE_API_KEY)

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=OPENAI_API_KEY
)

index_name = "clipoframeaiapp04"  # your existing index


def make_id(text: str) -> str:
    """Generate stable ID for deduplication in Pinecone."""
    return hashlib.md5(text.encode("utf-8")).hexdigest()


def store_context(video_id: str, text: str):
    """Store transcript or summary in Pinecone under per-video namespace."""
    index = pc.Index(index_name)
    vec = embeddings.embed_query(text)
    uid = make_id(text)
    index.upsert(
        [{"id": uid, "values": vec, "metadata": {"video_id": video_id, "text": text}}],
        namespace=video_id
    )
    print(f"[store_context] Stored doc id={uid} under namespace={video_id}")
    return uid  # ✅ return ID for tracking


def retrieve_context(video_id: str, query: str = "Extract key highlights", k: int = 20):
    """Retrieve docs from Pinecone by semantic similarity (per-video namespace)."""
    index = pc.Index(index_name)
    query_vec = embeddings.embed_query(query)
    res = index.query(
        namespace=video_id,
        vector=query_vec,
        top_k=k,
        include_metadata=True
    )
    matches = res.get("matches", [])
    print(f"[retrieve_context] Retrieved {len(matches)} docs under namespace={video_id} with query='{query}'")

    # Return structured dicts instead of raw strings
    return [
        {
            "text": m["metadata"].get("text", ""),
            "metadata": m["metadata"]
        }
        for m in matches if "metadata" in m
    ]


# ============================================
# STEP 9b — Memory Manager wrapper
# ============================================
class MemoryManager:
    def __init__(self, index_name, embeddings):
        self.index_name = index_name
        self.embeddings = embeddings

    def store_transcript(self, transcript, segments=None, video_id=None, title=None):
        return store_context(video_id, transcript)

    def store_summary(self, summary, video_id=None):
        return store_context(video_id, summary)

    def store_highlights(self, highlights, video_id=None):
        text = "\n".join(highlights)
        return store_context(video_id, text)

    def retrieve_context(self, query, video_id=None, k=10):
        return retrieve_context(video_id, query, k)


# Initialize global memory object
memory = MemoryManager(index_name, embeddings)


# ============================================
# STEP 9c — Global Fact Storage / Recall
# ============================================
def store_fact(fact_text: str, metadata: dict = None):
    """Store a single fact in global memory (namespace='facts')."""
    index = pc.Index(index_name)
    vec = embeddings.embed_query(fact_text)
    md = metadata or {}
    md.update({"type": "fact", "date": datetime.utcnow().isoformat()})
    uid = make_id(fact_text)
    index.upsert([{"id": uid, "values": vec, "metadata": {"text": fact_text, **md}}], namespace="facts")
    print(f"[store_fact] Stored fact id={uid}")
    return uid


def recall_facts(query: str, top_k: int = 5, namespace: str = "facts"):
    """Recall stored facts from global memory."""
    index = pc.Index(index_name)
    query_vec = embeddings.embed_query(query)
    res = index.query(namespace=namespace, vector=query_vec, top_k=top_k, include_metadata=True)
    matches = res.get("matches", [])
    print(f"[recall_facts] Retrieved {len(matches)} facts")
    return [m["metadata"].get("text", "") for m in matches if "metadata" in m]

# ================================
# SEED: App knowledge -> Pinecone
# Run after STEP 9 (pc, index_name, embeddings, make_id exist)
# ================================
# ================================
# SEED: App + Creator Knowledge -> Pinecone (with platform tags)
# Run after STEP 9 (pc, index_name, embeddings, make_id exist)
# ================================
from datetime import datetime
import json

# --- 1) Configure --- #
INDEX = pc.Index(index_name)
NAMESPACES = ["app_info", "facts"]
BATCH_SIZE = 50

# --- 2) Prepare seed content --- #
seed_docs = {
    "about": [
        ("ClipoFrameAI is a dark-themed AI web app that transforms long videos into instant summaries, highlights, and Q&A via a chat-style interface.", "general"),
        ("The product merges chat-driven video understanding with short-form monetization features like auto-clipping for TikTok/Shorts.", "general"),
    ],
    "features": [
        ("Dark Theme UI with React + TailwindCSS + Framer Motion.", "general"),
        ("Voice Input using Whisper for speech-to-text transcription.", "general"),
        ("Embedded YouTube player with clickable highlights and timestamps.", "general"),
        ("Chat-style QA interface to ask natural language questions about any video.", "general"),
        ("Multilingual support and RAG-powered retrieval (LangChain + Pinecone/Chroma).", "general"),
        ("Auto-clipping mode to export short clips for TikTok/YouTube Shorts.", "general"),
        ("Built-in assistant to guide users on usage, content ideas, and app features.", "general"),
    ],
    "how_to_use": [
        ("Paste a YouTube link into the pipeline (or upload a file) and choose a task: Transcript, Summarize, Highlights, Q&A, or Viral Captions.", "general"),
        ("After the pipeline runs, switch to the Jarvis Chat tab and ask 'Summarize this' or ask any question about the processed video.", "general"),
        ("To process private videos, provide an up-to-date cookies.txt file and use refresh_cookies() when the cookie expires.", "general"),
    ],
    "getting_started": [
        ("Frontend: cd frontend && npm install && npm run dev (default host: http://localhost:5173).", "general"),
        ("Backend: cd backend && pip install -r requirements.txt && uvicorn app:app --reload (default host: http://localhost:8000).", "general"),
    ],
    "tech_stack": [
        ("Frontend: React + Vite + TailwindCSS + Framer Motion.", "general"),
        ("Backend: FastAPI, LangChain, Whisper, OpenAI/HuggingFace LLMs.", "general"),
        ("Vector DB: Pinecone (serverless) or Chroma; embeddings from OpenAI text-embedding-3-small.", "general"),
    ],
    "project_structure": [
        ("project/frontend — React app with components, pages, and services.", "general"),
        ("project/backend — FastAPI server with rag_pipeline.py, youtube_utils.py, app.py and requirements.txt.", "general"),
    ],
    "ui_flow": [
        ("Before login: hero section with paste link CTA, demo preview and signup/promo.", "general"),
        ("After login: Dashboard with embedded player + timeline highlights + chat assistant sidebar.", "general"),
    ],
    "pricing": [
        ("Free Tier: limited features and duration (e.g., 10-min summaries + basic chat).", "general"),
        ("Pro Tier: unlimited length, quiz mode, batch processing, auto-clips for TikTok/Shorts (e.g., $10/month).", "general"),
    ],
    "monetization": [
        ("Free tier to attract users; Pro subscriptions for power users.", "general"),
        ("Creator growth: share templates & captions; in-app viral content to drive ad revenue.", "general"),
    ],
    "faq": [
        ("Q: How do I get highlights? A: Run the pipeline with the Highlights task or ask Jarvis 'Show highlights' after processing a video.", "general"),
        ("Q: How to use private videos? A: Upload a fresh cookies.txt and use refresh_cookies('cookies.txt') in Colab to allow yt-dlp access.", "general"),
    ],
    "troubleshooting": [
        ("If Whisper import fails due to numba/numpy version issues, install openai-whisper and a compatible numba/numpy.", "general"),
        ("If yt-dlp fails with Login required, refresh your cookies.txt and run refresh_cookies() in the notebook.", "general"),
        ("If Pinecone upserts fail, ensure PINECONE_API_KEY env var is set and pc client was created.", "general"),
    ],
    "developer_notes": [
        ("Keep 'memory = MemoryManager(index_name, embeddings)' in your notebook so tools can use the MemoryManager wrapper.", "general"),
        ("Keep a consistent 'index_name' (clipoframeaiapp04) across all cells — it's used by pc.Index(index_name).", "general"),
        ("Seeding uses stable ids (md5) so re-running will update existing entries rather than duplicate them.", "general"),
    ],
    # --- NEW: Creator Tips ---
    "creator_tips": [
        ("For TikTok/Reels/Shorts: Keep videos between 15-30 seconds for virality. Start with a strong hook in the first 3 seconds.", "tiktok"),
        ("Always use captions and bold text overlays in short-form videos. Most users watch muted.", "general"),
        ("TikTok/Reels/Shorts should be vertical 9:16 (1080x1920). Post 2-3 clips daily for fast growth.", "tiktok"),
        ("On YouTube, 8–12 minutes works best for monetization. Structure: Hook → Story → Payoff → Call to Action.", "youtube"),
        ("Use YouTube Shorts to funnel viewers into your full videos. Pair Shorts with strong titles and thumbnails.", "youtube"),
        ("On Instagram Reels, keep it under 60 seconds. Add engaging captions and trending sounds for better reach.", "instagram"),
        ("Collaborations on Instagram (tagging other creators) helps reach new audiences faster.", "instagram"),
        ("Best content types: Educational nuggets, reaction clips, behind-the-scenes, storytelling, and listicles.", "general"),
        ("Growth hack: Repurpose one long video into 10+ short clips for TikTok, Reels, and Shorts.", "general"),
        ("Use AI captions with emojis and bold keywords to increase retention.", "general"),
        ("Always end videos with a strong CTA: follow, subscribe, or comment.", "general"),
        ("Engage with comments within the first hour of posting to boost algorithm visibility.", "general"),
        ("Cross-post content across TikTok, Instagram Reels, YouTube Shorts, and Twitter for maximum reach.", "general"),
    ],
    # --- NEW: Monetization Tips ---
    "monetization_tips": [
        ("TikTok Creator Fund pays per view but limited. Better monetization comes from brand deals and UGC content.", "tiktok"),
        ("YouTube Partner Program monetizes long-form content. Focus on audience retention and watch time.", "youtube"),
        ("Creators can monetize by selling digital products like courses, templates, and presets.", "general"),
        ("Brand collaborations and sponsorships are often more lucrative than ad revenue.", "general"),
        ("Affiliate marketing is an effective way to monetize audiences across platforms.", "general"),
    ]
}

# --- 3) Flatten & embed --- #
items = []
for category, texts in seed_docs.items():
    for t, platform in texts:
        uid = make_id(f"{category}|{t[:300]}")
        md = {
            "source": "seed",
            "category": category,
            "platform": platform,
            "text": t,
            "date": datetime.utcnow().isoformat()
        }
        items.append({"id": uid, "text": t, "metadata": md})

def embed_texts(text_list):
    try:
        return embeddings.embed_documents(text_list)
    except Exception:
        return [embeddings.embed_query(tx) for tx in text_list]

texts = [it["text"] for it in items]
vectors = embed_texts(texts)

payloads = []
for i, it in enumerate(items):
    payloads.append({
        "id": it["id"],
        "values": vectors[i],
        "metadata": it["metadata"]
    })

print(f"[seed] Prepared {len(payloads)} payloads. Upserting in batches...")

for ns in NAMESPACES:
    for i in range(0, len(payloads), BATCH_SIZE):
        batch = payloads[i:i+BATCH_SIZE]
        INDEX.upsert(batch, namespace=ns)
    print(f"[seed] Upserted {len(payloads)} docs under namespace={ns}")

with open("app_seeded_docs.jsonl", "w", encoding="utf-8") as fout:
    for p in payloads:
        fout.write(json.dumps(p["metadata"], ensure_ascii=False) + "\n")

print("✅ Seeding complete. Jarvis can now recall app info + creator/monetization tips with platform tags.")



# ============================================
# STEP 10 — RLHF + store interactions long-term
# ============================================
RLHF_PATH = "rlhf_data.jsonl"
if not os.path.exists(RLHF_PATH):
    open(RLHF_PATH, "w").close()

def store_interaction_longterm(prompt_text, response_text, metadata=None, namespace="global_interactions"):
    try:
        index = pc.Index(index_name)
        vec = embeddings.embed_query(f"Q: {prompt_text}\nA: {response_text}")
        uid = make_id(prompt_text + response_text)
        index.upsert(
            [{"id": uid, "values": vec, "metadata": {"prompt": prompt_text, "response": response_text}}],
            namespace=namespace
        )
    except Exception as e:
        print("Warning: couldn't upsert to Pinecone:", e)

    record = {
        "timestamp": datetime.utcnow().isoformat(),
        "prompt": prompt_text,
        "response": response_text,
        "metadata": metadata or {}
    }
    with open(RLHF_PATH, "a") as f:
        f.write(json.dumps(record) + "\n")

def add_feedback_to_latest_entry(feedback_bool):
    with open(RLHF_PATH, "r") as f:
        lines = [json.loads(l) for l in f if l.strip()]
    if not lines:
        return False
    for i in range(len(lines) - 1, -1, -1):
        if lines[i].get("feedback") is None:
            lines[i]["feedback"] = bool(feedback_bool)
            break
    with open(RLHF_PATH, "w") as f:
        for rec in lines:
            f.write(json.dumps(rec) + "\n")
    return True

# ============================================
# STEP 11 — Jarvis Agent Tools (updated to use dict outputs from retrieve_context)
# ============================================

def _tool_transcribe(path_or_url: str):
    """Transcribes a path or URL, stores it in Pinecone, and returns video_id and preview."""
    try:
        if path_or_url.startswith("http"):
            out = download_youtube_audio(path_or_url)
            transcript, segments = transcribe_file(out)
        else:
            transcript, segments = transcribe_file(path_or_url)

        vid = f"video_{int(time.time())}"
        title = path_or_url if path_or_url.startswith("http") else os.path.basename(path_or_url)

        memory.store_transcript(transcript, video_id=vid)
        store_interaction_longterm(f"Transcribe:{path_or_url}", "stored", {"video_id": vid})

        return {"video_id": vid, "preview": transcript[:800]}
    except Exception as e:
        return {"error": str(e)}


def _tool_summarize(arg: str):
    """Summarize a video_id or text using Pinecone retrieval."""
    if isinstance(arg, str) and arg.startswith("video_"):
        vid = arg
        docs = retrieve_context(vid, query="Summarize this video", k=20)
        combined_text = "\n\n".join([d["text"] for d in docs]) if docs else ""
        if not combined_text:
            return "No transcript chunks found for that video in memory."

        result = summarize_transcript(combined_text)
        memory.store_summary(result, video_id=vid)
    else:
        result = summarize_transcript(arg)

    store_interaction_longterm(f"summarize:{arg}", result, {"video_id": arg if isinstance(arg, str) and arg.startswith("video_") else None})
    return result


def _tool_highlights(arg: str):
    """Extract highlights from a video_id or raw text using Pinecone retrieval."""
    try:
        if isinstance(arg, str) and arg.startswith("video_"):
            vid = arg
            docs = retrieve_context(vid, query="Extract highlights with timestamps", k=30)
            context_segments = [d["text"] for d in docs]
            if not context_segments:
                return {"error": "no_segments_found", "raw": "No transcript segments found in memory for this video."}

            highlight_text_for_gpt = "\n".join(context_segments)
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a highlights extractor for videos. Respond in JSON array format."},
                    {"role": "user", "content": f"Extract up to 5 highlights with timestamps from this:\n\n{highlight_text_for_gpt}"}
                ],
                max_tokens=700,
                temperature=0.1
            )
            raw_output = response.choices[0].message.content
            parsed_highlights = safe_json_parse(raw_output)

            memory.store_highlights([h.get("text", "") for h in parsed_highlights if h.get("text")], video_id=vid)
            return parsed_highlights
        else:
            return call_gpt_for_highlights([{"start": 0, "end": 0, "text": arg[:2000]}])
    except Exception as e:
        return {"error": "highlights_failed", "raw": str(e)}


def _tool_qa(qtext: str):
    """Answer questions using video context or global memory."""
    video_id = None
    q = qtext

    if "|" in qtext and qtext.split("|", 1)[0].startswith("video_"):
        vid, q = qtext.split("|", 1)
        video_id = vid.strip()

    if video_id:
        docs = retrieve_context(video_id, query=q, k=8)
        context = "\n\n".join([d["text"] for d in docs]) if docs else ""
    else:
        facts = recall_facts(q, top_k=5, namespace="facts")
        context = "\n\n".join([f["metadata"].get("text", f.get("text", "")) for f in facts]) if facts else ""

    prompt = f"Context:\n{context}\n\nQuestion: {q}\nAnswer concisely."
    ans = gpt4_chat(prompt, system="You are Jarvis, expert assistant.", max_tokens=600)

    store_interaction_longterm(q, ans, {"video_id": video_id})
    return ans



def _tool_viral(arg: str):
    """Generate viral ideas from a video_id or raw text."""
    if isinstance(arg, str) and arg.startswith("video_"):
        vid = arg
        docs = retrieve_context(vid, query="Suggest viral clip ideas", k=10)
        text = "\n\n".join([d["text"] for d in docs]) if docs else ""
        if not text:
            return "No transcript chunks found for that video in memory."
    else:
        text = arg

    ideas = generate_viral_ideas(text)
    store_interaction_longterm(
        f"viral:{arg}",
        ideas,
        {"video_id": arg if arg.startswith("video_") else None}
    )
    return ideas



def _tool_creator_tips(query: str, platform: str = None, k: int = 5):
    """
    Retrieve creator/monetization tips.
    - query: user question, e.g. "How do I grow on TikTok?"
    - platform: optional filter ("tiktok", "youtube", "instagram", "general")
    """
    try:
        # Retrieve from global facts namespace
        docs = recall_facts(query, top_k=k, namespace="facts")

        # Filter by platform if provided
        filtered = []
        for d in docs:
            md = d.metadata if hasattr(d, "metadata") else d.get("metadata", {})
            plat = md.get("platform")
            txt = getattr(d, "page_content", None) or md.get("text")
            if txt and (not platform or plat == platform):
                filtered.append({"platform": plat, "tip": txt})

        if not filtered:
            return {"error": "no_tips_found", "raw": f"No tips found for {platform or 'any platform'}."}

        # Store in long-term log
        store_interaction_longterm(f"creator_tips:{query}", filtered, {"platform": platform})
        return filtered
    except Exception as e:
        return {"error": "creator_tips_failed", "raw": str(e)}

# ============================================
# STEP 12 — Pipeline core returning (result, context)
# - context contains video_id, transcript, segments
# ============================================

def pipeline_core(video_file, video_url, task, question=""):
    try:
        if video_url:
            fpath = download_youtube_audio(video_url)
            transcript, segments = transcribe_file(fpath)
            title = video_url
        elif video_file:
            transcript, segments = transcribe_file(video_file)
            title = os.path.basename(video_file)
        else:
            return "Please provide a URL or upload a file.", {"video_id": None}

        vid = f"video_{int(time.time())}"

        # Store transcript in Pinecone (via MemoryManager wrapper)
        try:
            memory.store_transcript(transcript, segments, video_id=vid, title=title)
        except Exception as e:
            print("Warning store transcript:", e)

        # Handle task types
        result = None
        if task == "Transcript":
            result = transcript

        elif task == "Summarize":
            docs = retrieve_context(vid, query="Summarize this video", k=20)
            combined_text = "\n\n".join([d["text"] for d in docs]) if docs else transcript
            summary = summarize_transcript(combined_text)
            try:
                memory.store_summary(summary, video_id=vid)
            except Exception as e:
                print("Warning store summary:", e)
            result = summary

        elif task == "Highlights":
            highlights_res = _tool_highlights(vid)
            result = highlights_res

        elif task == "Q&A":
            result = _tool_qa(f"{vid}|{question}")

        elif task == "Viral Captions":
            result = generate_viral_ideas(transcript)

        else:
            result = "Invalid task."

        # Attach context for Jarvis chat continuity
        context = {"video_id": vid, "transcript": transcript, "segments": segments, "title": title}
        return result, context

    except Exception as e:
        return f"Pipeline error: {e}", {"video_id": None}



def jarvis_entry_core(user_text, audio_file, uploaded_file, context):
    """
    Hybrid behavior:
      - If user pastes URL in chat, we process it right away (transcribe & store).
      - If context contains video_id, common intents go to tools (summarize/highlights/qa).
      - Otherwise agent falls back to global recall or general LLM answer using Pinecone retrieval.
    """
    input_text = (user_text or "").strip()

    # If voice/file - transcribe first
    if audio_file:
        try:
            t, segs = transcribe_file(audio_file)
            input_text = t
        except Exception as e:
            return f"Audio transcription error: {e}"

    if uploaded_file and not input_text:
        try:
            t, segs = transcribe_file(uploaded_file)
            input_text = t
        except Exception as e:
            return f"Upload transcription error: {e}"

    # If user pasted a YouTube URL directly
    if input_text.startswith("http") and ("youtube.com" in input_text or "youtu.be" in input_text):
        try:
            trans_res = _tool_transcribe(input_text)
            vid = trans_res.get("video_id") if isinstance(trans_res, dict) else None
            preview = (trans_res.get("preview")[:800] + "...") if trans_res and trans_res.get("preview") else ""
            return f"Processed URL and stored as {vid}. Preview:\n\n{preview}"
        except Exception as e:
            return f"Error processing URL: {e}"

    # If a video context exists → route to tools
    if context and isinstance(context, dict) and context.get("video_id"):
        vid = context.get("video_id")
        if not vid:
            return "⚠️ No video context available — please run the pipeline first."
        low = input_text.lower()
        if low in ["summarize", "summarize this", "summarise"]:
            return _tool_summarize(vid)
        if "highlight" in low:
            res = _tool_highlights(vid)
            return res.get("html") if isinstance(res, dict) else str(res)
        if "viral" in low or "tiktok" in low or "short" in low:
            return _tool_viral(vid)
        if "?" in input_text or any(k in low for k in ["what","who","how","when","where","why"]):
            return _tool_qa(f"{vid}|{input_text}")

    # Fallback: use agent or recall facts
    if jarvis_agent:
        try:
            resp = jarvis_agent.run(input_text)
        except Exception as e:
            resp = f"Agent error: {e}"
    else:
        # Try semantic search across global facts
        docs = recall_facts(input_text, top_k=5, namespace="facts")
        ctx = "\n\n".join([d["metadata"].get("text", d.get("text", "")) for d in docs]) if docs else ""

        if ctx:
            prompt = f"Context:\n{ctx}\n\nQuestion: {input_text}\nAnswer helpfully."
            resp = gpt4_chat(prompt, system="You are Jarvis", max_tokens=400)
        else:
            # Last resort: plain LLM
            resp = gpt4_chat(input_text, system="You are Jarvis", max_tokens=400)

    # Store interaction for RLHF + long-term memory
    store_interaction_longterm(input_text, resp, {"source": "jarvis_interaction"})
    return resp

# ============================================
# STEP 14 — Agent Init with Tools + Memory
# ============================================
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, Tool
from langchain.memory import ConversationBufferMemory

chat_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
agent_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.4, openai_api_key=OPENAI_API_KEY)

tools = [
    Tool(name="Transcribe", func=_tool_transcribe, description="Transcribe audio/video or YouTube URL"),
    Tool(name="Summarize", func=_tool_summarize, description="Summarize transcript or video"),
    Tool(name="Highlights", func=_tool_highlights, description="Extract highlights with timestamps"),
    Tool(name="Q&A", func=_tool_qa, description="Answer questions based on video context or facts"),
    Tool(name="Viral Captions", func=_tool_viral, description="Generate viral captions and content ideas"),
    Tool(name="Creator Tips", func=_tool_creator_tips, description="Get platform-specific growth/monetization tips for TikTok, YouTube, Instagram"),
]


jarvis_agent = initialize_agent(
    tools,
    agent_llm,
    agent="conversational-react-description",
    memory=chat_memory,
    verbose=True
)
print("✅ Jarvis agent initialized")

# ============================================
# STEP PUSH — HF push + local export helpers
# ============================================
from huggingface_hub import HfApi, login, Repository

def export_whisper_placeholder(model_name=WHISPER_MODEL, out_dir="/content/models/whisper"):
    """Creates a small placeholder instead of pushing full Whisper weights."""
    os.makedirs(out_dir, exist_ok=True)
    with open(os.path.join(out_dir, "METADATA.txt"), "w") as f:
        f.write(f"Whisper model: {model_name}\nUse whisper.load_model('{model_name}') in production.")
    return out_dir

def export_embeddings_placeholder(out_dir="/content/models/embeddings"):
    """Creates a small placeholder instead of uploading full embeddings model."""
    os.makedirs(out_dir, exist_ok=True)
    with open(os.path.join(out_dir, "README.txt"), "w") as f:
        f.write("Embeddings are generated via OpenAI API; store API key to recompute.")
    return out_dir

def push_artifacts_to_hf(local_dir="clipo_repo", repo_name="ClipoFrameAI"):
    """Pushes lightweight placeholders or artifacts to a private HF repo."""
    if not HF_TOKEN:
        return "HF token missing ❌"

    # Configure git identity (Colab fix)
    os.system('git config --global user.email "saifobaidi.s22@gmail.com"')
    os.system('git config --global user.name "Samobai"')

    login(token=HF_TOKEN)
    api = HfApi()
    user = api.whoami()["name"]
    repo_id = f"{user}/{repo_name}"
    api.create_repo(repo_id=repo_id, private=True, exist_ok=True)

    repo = Repository(local_dir=local_dir, clone_from=repo_id, use_auth_token=HF_TOKEN)
    os.makedirs(local_dir, exist_ok=True)
    with open(os.path.join(local_dir, "pipeline_placeholder.py"), "w") as f:
        f.write("# ClipoFrameAI pipeline placeholder\n")
    repo.push_to_hub(commit_message=f"update {int(time.time())}")
    return f"https://huggingface.co/{repo_id}"

# ============================================
# STEP 14b — Simple Remember / Recall Tools
# ============================================

def tool_remember(fact: str):
    """Store a fact into long-term memory (namespace='facts')."""
    if not fact.strip():
        return "⚠️ Nothing to remember."
    store_fact(fact, metadata={"source": "user"})
    return f"✅ Remembered: {fact}"

def tool_recall(query: str):
    """Recall facts from long-term memory."""
    if not query.strip():
        return "⚠️ Empty recall query."
    matches = recall_facts(query, top_k=5, namespace="facts")
    if not matches:
        return "❌ No matching facts found."
    return "\n".join([m["metadata"].get("text", str(m)) if "metadata" in m else str(m) for m in matches])

# ============================================
# STEP  — RLHF feedback logger
# ============================================

import json

RLHF_FILE = "rlhf_data.jsonl"
_last_response = None  # global buffer to store latest Jarvis response

def store_interaction_longterm(prompt, response, metadata=None):
    """Store conversation turn (for RLHF and auditing)."""
    global _last_response
    entry = {
        "timestamp": time.time(),
        "prompt": prompt,
        "response": response,
        "metadata": metadata or {}
    }
    with open(RLHF_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry) + "\n")
    _last_response = entry  # keep last response for feedback
    return True

def add_feedback_to_latest_entry(is_helpful: bool):
    """Attach human feedback (👍👎) to the last logged response."""
    global _last_response
    if not _last_response:
        return "⚠️ No recent response to give feedback on."
    fb_entry = dict(_last_response)
    fb_entry["feedback"] = "helpful" if is_helpful else "not_helpful"
    with open(RLHF_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(fb_entry) + "\n")
    return "✅ Feedback saved!"

# ============================================
# STEP 17 — Visualization, Analytics & LangSmith
# ============================================
import matplotlib.pyplot as plt
import json
from langsmith import Client

# Init LangSmith client
ls_client = Client(api_key=LANGSMITH_API_KEY)

def collect_memory_stats():
    index = pc.Index(index_name)
    stats = index.describe_index_stats()
    ns = stats.get("namespaces", {})
    fact_count = ns.get("facts", {}).get("vector_count", 0)
    video_counts = {k: v["vector_count"] for k, v in ns.items() if k.startswith("video_")}

    result = {
        "total_vectors": stats.get("total_vector_count", 0),
        "fact_count": fact_count,
        "videos": video_counts
    }
    # log to LangSmith
    ls_client.log_event(
        name="memory_stats",
        inputs={},
        outputs=result,
        metadata={"index": index_name}
    )
    return result

def collect_feedback_stats(file=RLHF_FILE):
    helpful, not_helpful = 0, 0
    try:
        with open(file, "r", encoding="utf-8") as f:
            for line in f:
                entry = json.loads(line)
                if entry.get("feedback") == "helpful":
                    helpful += 1
                elif entry.get("feedback") == "not_helpful":
                    not_helpful += 1
    except FileNotFoundError:
        pass

    result = {"helpful": helpful, "not_helpful": not_helpful}
    # log to LangSmith
    ls_client.log_event(
        name="feedback_stats",
        inputs={},
        outputs=result,
        metadata={"file": file}
    )
    return result

def plot_memory_growth():
    stats = collect_memory_stats()
    videos = stats["videos"]
    plt.figure(figsize=(6,4))
    plt.bar(videos.keys(), videos.values())
    plt.xticks(rotation=45, ha="right")
    plt.title("Vectors per Video Namespace")
    plt.tight_layout()
    return plt, stats

def plot_feedback_distribution():
    fb = collect_feedback_stats()
    plt.figure(figsize=(4,4))
    plt.pie(
        [fb["helpful"], fb["not_helpful"]],
        labels=["👍 Helpful", "👎 Not Helpful"],
        autopct="%1.1f%%"
    )
    plt.title("RLHF Feedback Distribution")
    return plt, fb

# ============================================
# STEP 15 — Gradio UI with shared context_state + Analytics
# ============================================
import gradio as gr

with gr.Blocks(title="ClipoFrameAI — Jarvis Brain (Long-term Memory)") as demo:
    gr.Markdown("# 🎬 ClipoFrameAI — Jarvis with Long-Term Memory")

    # Shared state between pipeline + chat
    context_state = gr.State(
        value={"video_id": None, "transcript": None, "segments": None, "title": None}
    )

    # ---------------- Classic Pipeline Tab ----------------
    with gr.Tab("Classic Pipeline"):
        with gr.Row():
            with gr.Column(scale=2):
                url_in = gr.Textbox(label="Paste YouTube URL", placeholder="https://youtu.be/...")
                upload_file = gr.File(label="Or Upload Video/Audio", type="filepath")
                task = gr.Radio(
                    ["Transcript", "Summarize", "Highlights", "Q&A", "Viral Captions"],
                    label="Choose Task"
                )
                question = gr.Textbox(label="Question (for Q&A)")
                run_btn = gr.Button("Run Task")
                pipeline_out = gr.Textbox(label="Result", lines=12)
                highlights_html = gr.HTML(label="Highlights (clickable)")
            with gr.Column(scale=1):
                gr.Markdown(
                    "**Tips**: After processing, switch to Jarvis tab and ask "
                    "'Summarize this' or any question."
                )
                export_btn = gr.Button("Export Artifacts to HF (placeholder)")
                export_status = gr.Textbox(label="Export status", lines=2)

        # Wrapper for pipeline_core
        def pipeline_core_with_html(video_file, video_url, task_in, question_in):
            res, ctx = pipeline_core(video_file, video_url, task_in, question_in)
            if task_in == "Highlights":
                if isinstance(res, dict) and res.get("html"):
                    return "", res.get("html"), ctx
                if isinstance(res, list):
                    return "", highlights_to_html(res), ctx
            return str(res), "", ctx

        run_btn.click(
            fn=pipeline_core_with_html,
            inputs=[upload_file, url_in, task, question],
            outputs=[pipeline_out, highlights_html, context_state]
        )

        export_btn.click(
            fn=lambda: push_artifacts_to_hf(),
            inputs=None,
            outputs=export_status
        )

    # ---------------- Jarvis Chat Tab ----------------
    with gr.Tab("Jarvis (Chat)"):
        with gr.Row():
            with gr.Column(scale=2):
                user_text = gr.Textbox(
                    label="Type or paste command/chat",
                    placeholder="e.g. Summarize this or 'How many highlights?'"
                )
                audio_in = gr.Audio(sources=["microphone"], type="filepath", label="Or speak (voice)")
                file_in = gr.File(label="Or upload audio/video", type="filepath")
                send_btn = gr.Button("Send to Jarvis")
                jarvis_out = gr.Textbox(label="Jarvis Response", lines=12)
                like_btn = gr.Button("👍 Helpful")
                dislike_btn = gr.Button("👎 Not Helpful")
                remember_text = gr.Textbox(
                    label="Remember (short fact)",
                    placeholder="Type a short fact to store in long-term memory"
                )
                remember_btn = gr.Button("Remember this")
                recall_query = gr.Textbox(label="Recall query", placeholder="Search your long-term memory")
                recall_btn = gr.Button("Recall")
                recall_out = gr.Textbox(label="Recall results", lines=8)
            with gr.Column(scale=1):
                gr.Markdown(
                    "**Agent Tools**\n"
                    "- 'Summarize this' (after running pipeline)\n"
                    "- 'What was this video about?'\n"
                    "- Paste link to process it directly"
                )

        # Wrapper for chat
        def jarvis_entry_with_context(user_text_in, audio_file_in, uploaded_file_in, current_context_state):
            return jarvis_entry_core(user_text_in, audio_file_in, uploaded_file_in, current_context_state)

        send_btn.click(
            fn=jarvis_entry_with_context,
            inputs=[user_text, audio_in, file_in, context_state],
            outputs=jarvis_out
        )
        like_btn.click(fn=lambda: add_feedback_to_latest_entry(True), inputs=None, outputs=jarvis_out)
        dislike_btn.click(fn=lambda: add_feedback_to_latest_entry(False), inputs=None, outputs=jarvis_out)
        remember_btn.click(fn=tool_remember, inputs=[remember_text], outputs=[jarvis_out])
        recall_btn.click(fn=tool_recall, inputs=[recall_query], outputs=[recall_out])

    # ---------------- Analytics Tab ----------------
    with gr.Tab("Analytics"):
        with gr.Row():
            with gr.Column():
                gr.Markdown("### 📊 Memory Growth")
                mem_plot = gr.Plot()
                mem_json = gr.JSON()
                mem_btn = gr.Button("Refresh Memory Stats")
            with gr.Column():
                gr.Markdown("### 👍👎 Feedback Distribution")
                fb_plot = gr.Plot()
                fb_json = gr.JSON()
                fb_btn = gr.Button("Refresh Feedback Stats")

        # Hook up memory stats
        def refresh_memory():
            plt, stats = plot_memory_growth()
            return plt, stats
        mem_btn.click(fn=refresh_memory, inputs=None, outputs=[mem_plot, mem_json])

        # Hook up feedback stats
        def refresh_feedback():
            plt, fb = plot_feedback_distribution()
            return plt, fb
        fb_btn.click(fn=refresh_feedback, inputs=None, outputs=[fb_plot, fb_json])

    # Footer
    gr.Markdown("RLHF file: `rlhf_data.jsonl`")
    gr.Markdown(f"Pinecone index: `{index_name}`")

# %%
# ============================================
# STEP 16 — Launch demo (share=True if public URL needed)
# ============================================
demo.launch(debug=True, share=False)